{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision.transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_LOGGING = False\n",
    "config = {\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Transforming the Dataset\n",
    "\n",
    "In this project we are working with imamges of vegetables and classifying them into 15 distinct classes.\n",
    "\n",
    "The images were downloaded from: https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset. There are 21,000 images in total. The images are already split into 3 sets - training(70%), validation(15%) and testing(15%). However, the percentages (provided on the website) are only rough estimations - in actuality, there are 15,000 training images, and 3000 of each - validation and testing images - which makes it a 71,4285...%, 14,2857...%, 14,2857...% split...\n",
    "\n",
    "We begin by loading the images from the downloaded repository using the `ImageFolder` object from the `torchvision.datasets` module, immediately giving them transformations defined in the two following blocks of code. First of all, we need to make sure that the images are the same size, so we resize all of them to 224x224 pixels. In addition to that, we augment the training images by doing random rotations and random resized crops (within a range of 80-100% of the original image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                     [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                      transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       #transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                                                            #[0.5, 0.5, 0.5])])\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder('data/Vegetable Images/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('data/Vegetable Images/validation', transform=transform)\n",
    "test_dataset = ImageFolder('data/Vegetable Images/test', transform=transform)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"dataloader\"][\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config[\"dataloader\"][\"batch_size\"], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config[\"dataloader\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(f\"Number of Training Images: {len(train_dataset)} - Validation Images: {len(val_dataset)} - Test Images: {len(test_dataset)}\")\n",
    "print(f\"Number of target classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Names of the classes: {(train_dataset.classes)}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a training image after augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "fn.to_pil_image(images[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VegetableClassifier(nn.Module):\n",
    "    def __init__(self, network: nn.Sequential, network_out: int, num_classes: int=15) -> None:\n",
    "        super(VegetableClassifier, self).__init__()\n",
    "        self.classifier_out = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.network = network\n",
    "        self.avg_maxpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.linear_classifier = nn.Linear(in_features=network_out, out_features=self.classifier_out)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.network(input)\n",
    "        input = self.avg_maxpool(input)\n",
    "        input = torch.flatten(input, 1)\n",
    "        return self.linear_classifier(input)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_2 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "if WANDB_LOGGING:\n",
    "    sweep_configuration = {\n",
    "    'method': 'random',\n",
    "    'name': 'sweep',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'validation_loss'\n",
    "        },\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10, 15]},\n",
    "        'lr': {'max': 0.1, 'min': 0.0001}\n",
    "     }\n",
    "    }\n",
    "    wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # We use tqdm to display a simple progress bar, allowing us to observe the learning progression.\n",
    "\n",
    "def get_accuracy(pred, y):\n",
    "    pred = pred.argmax(dim=1) # Get the largest value for each image - the predicted class\n",
    "    correct = torch.eq(y, pred).sum().item()\n",
    "    acc = (correct / len(pred)) * 100 # \n",
    "    return acc\n",
    "\n",
    "def fit(model: nn.Module,\n",
    "        device: torch.device,\n",
    "        num_epochs: int,\n",
    "          loss_fn,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "              train_dataloader: DataLoader,\n",
    "                test_dataloader: DataLoader,\n",
    "                  print_rate: int = 100):\n",
    "    # Iterate through epochs with tqdm\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch: {epoch}\\n\")\n",
    "        # Setup train loss that will accumulate over each batch\n",
    "        train_loss = 0\n",
    "        # Iterate through batches in each epoch\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            # Pass the tensors to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Set mode of model to train\n",
    "            model.train()\n",
    "            # Get the loss\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss\n",
    "            # Getting the loss gradient and making an optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch % print_rate == 0) or (batch == len(train_dataloader) - 1): \n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{batch * len(X)}/{len(train_dataloader.dataset)} Samples\")\n",
    "        # Devide the train_loss by the number of batches to get the average train_loss\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        # Testing\n",
    "        # Setup the Test Loss and Accuracy to accumulate over the batches in the test data set\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        # Set model to evaluation mode and use torch.inference_mode to remove unnecessary training operations \n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X_test, y_test in test_dataloader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "                test_pred = model(X_test)\n",
    "                test_loss += loss_fn(test_pred, y_test)\n",
    "                test_acc += get_accuracy(pred=test_pred, y=y_test)\n",
    "\n",
    "        # Get the average Test Loss and Accuracy\n",
    "        avg_test_loss = test_loss / len(test_dataloader)\n",
    "        avg_test_acc = test_acc / len(test_dataloader)\n",
    "\n",
    "        print(f\"Train loss: {avg_train_loss} | Test Loss: {avg_test_loss} | Test Accuracy: {avg_test_acc}\")\n",
    "        if WANDB_LOGGING:\n",
    "            wandb.log({\"Train Loss\": avg_train_loss,\"Test Loss\": avg_test_loss, \"Test Accuracy\": avg_test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VegetableClassifier(network=network_1, network_out=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device=device)\n",
    "fit(model=model, num_epochs=5, device=device, loss_fn=loss_fn, optimizer=optimizer, train_dataloader=train_dataloader, test_dataloader=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
