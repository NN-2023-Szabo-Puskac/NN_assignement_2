{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision.transforms\n",
    "import torchmetrics\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "WANDB_LOGGING = True\n",
    "CONFIG = {\n",
    "    \"project_name\": \"viktor_test_runs\",\n",
    "    \"dataloader\": {\n",
    "        \"batch_size\": 96\n",
    "    },\n",
    "    \"bias\": True,\n",
    "    \"lr\": 0.0001\n",
    "}\n",
    "SWEEP_CONFIG = {\n",
    "    'method': 'random',\n",
    "    'name': 'sweep',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'validation_loss'\n",
    "        },\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10, 15]},\n",
    "        'lr': {'max': 0.1, 'min': 0.0001}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Transforming the Dataset\n",
    "\n",
    "In this project we are working with images of vegetables and classifying them into 15 distinct classes.\n",
    "\n",
    "The images were downloaded from: https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset. There are 21,000 images in total. The images are already split into 3 sets - training(70%), validation(15%) and testing(15%). However, the percentages (provided on the website) are only rough estimations - in actuality, there are 15,000 training images, and 3000 of each - validation and testing images - which makes it a 71,4285...%, 14,2857...%, 14,2857...% split...\n",
    "\n",
    "We begin by loading the images from the downloaded repository using the `ImageFolder` object from the `torchvision.datasets` module, immediately giving them transformations defined in the two following blocks of code. First of all, we need to make sure that the images are the same size, so we resize all of them to 224x224 pixels. In addition to that, we augment the training images by doing random rotations and random resized crops (within a range of 80-100% of the original image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = ImageFolder('data/Vegetable Images/train', transform=transform)\n",
    "dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataloader):\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in dataloader:\n",
    "        # Mean over batch, height and width, but not over the channels\n",
    "        channels_sum += torch.mean(data, dim=[0,2,3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "mean, std = get_mean_and_std(dataloader)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder('data/Vegetable Images/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('data/Vegetable Images/validation', transform=train_transform)\n",
    "test_dataset = ImageFolder('data/Vegetable Images/test', transform=train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"], shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CONFIG[\"dataloader\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(f\"Number of Training Images: {len(train_dataset)} - Validation Images: {len(val_dataset)} - Test Images: {len(test_dataset)}\")\n",
    "print(f\"Number of target classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Names of the classes: {(train_dataset.classes)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a training image after augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "fn.to_pil_image(images[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_classifier(input_shape: int, output_shape: int, hidden_dim: int = 32):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_features=input_shape, out_features=hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=hidden_dim, out_features=hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=hidden_dim, out_features=output_shape),\n",
    "        nn.Softmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(128),\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(256),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_2 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    \n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    \n",
    "    nn.Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_2_a = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    \n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    \n",
    "    nn.Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_3 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(128),\n",
    "\n",
    "    nn.Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(256),\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(512),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_4 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_5 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_6 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size= (7,7), stride=(2,2), padding=(3,3), bias=CONFIG['bias']),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=CONFIG['bias']),\n",
    "    nn.BatchNorm2d(64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VegetableClassifier(nn.Module):\n",
    "    def __init__(self, loss_fn: callable, network: nn.Sequential, network_out: int, num_classes: int=15) -> None:\n",
    "        super(VegetableClassifier, self).__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "        self.classifier_out = num_classes\n",
    "        self.device = DEVICE\n",
    "        self.network = network\n",
    "        self.avg_maxpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.linear_classifier = get_linear_classifier(network_out, self.classifier_out)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.network(input)\n",
    "        input = self.avg_maxpool(input)\n",
    "        input = torch.flatten(input, 1)\n",
    "        return self.linear_classifier(input)\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        y_pred = self(X)\n",
    "        return self.loss_fn(y_pred, y)\n",
    "    \n",
    "    def val_step(self, X, y, accuracy):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        val_pred = self(X)\n",
    "        return self.loss_fn(val_pred, y), accuracy(val_pred, y)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # We use tqdm to display a simple progress bar, allowing us to observe the learning progression.\n",
    "\n",
    "def get_accuracy(pred, y):\n",
    "    pred = pred.argmax(dim=1) # Get the largest value for each image - the predicted class\n",
    "    correct = torch.eq(y, pred).sum().item()\n",
    "    acc = (correct / len(pred)) \n",
    "    return acc\n",
    "\n",
    "def fit(\n",
    "  model: nn.Module,\n",
    "  num_epochs: int,\n",
    "  optimizer: torch.optim.Optimizer,\n",
    "  train_dataloader: DataLoader,\n",
    "  val_dataloader: DataLoader,\n",
    "  print_rate: int = 100\n",
    "  ):\n",
    "    accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=model.classifier_out, average=\"weighted\").to(model.device)\n",
    "    model = model.to(model.device)\n",
    "    # Iterate through epochs with tqdm\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch: {epoch}\\n\")\n",
    "        train_loss = 0\n",
    "        model.train()  # Set mode of model to train\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            loss = model.train_step(X, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Getting the loss gradient and making an optimizer step\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % print_rate == 0: \n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{batch * len(X)}/{len(train_dataloader.dataset)} Samples\")\n",
    "            elif batch == len(train_dataloader) - 1:\n",
    "                print(f\"Looked at {batch} Batches\\t---\\t{len(train_dataloader.dataset)}/{len(train_dataloader.dataset)} Samples\")\n",
    "        \n",
    "        # Divide the train_loss by the number of batches to get the average train_loss\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        # Setup the Val Loss and Accuracy to accumulate over the batches in the val dataset\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        # Set model to evaluation mode and use torch.inference_mode to remove unnecessary training operations \n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X_val, y_val in val_dataloader:\n",
    "                loss, acc = model.val_step(X_val, y_val, accuracy)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += acc\n",
    "\n",
    "        # Get the average Val Loss and Accuracy\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        avg_val_acc = val_acc / len(val_dataloader)\n",
    "\n",
    "        print(f\"Train loss: {avg_train_loss} | Val Loss: {avg_val_loss} | Val Accuracy: {avg_val_acc}\")\n",
    "        if WANDB_LOGGING:\n",
    "            wandb.log({\"Train Loss\": avg_train_loss,\"Val Loss\": avg_val_loss, \"Val Accuracy\": avg_val_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model = VegetableClassifier(loss_fn=loss_fn, network=network_2, network_out=128)\n",
    "    print(f\"Total parameters: {sum(param.numel() for param in model.parameters())}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'], betas=(0.9, 0.99))\n",
    "    fit(model=model, num_epochs=10, optimizer=optimizer, train_dataloader=train_dataloader, val_dataloader=val_dataloader)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=CONFIG['project_name'])\n",
    "model = train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/network_2_lr_0001.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
